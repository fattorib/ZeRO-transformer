training:
  max_epochs: 10
  batch_size: 128
  peak_learning_rate: 1.3e-4
  warmup_steps: 2000
  decay_steps: 160000
  total_steps: 180000
  end_learning_rate: 2.5e-5
  weight_decay: 0.1
  gradient_accumulation_steps: 32
  evaluation_frequency: 500
  maximum_evaluation_steps: 500
  norm_log_frequency: 100
  precision: 'bf16'
  staged_warmup_steps: 98000
  warmup_train_context: 128

model:
  size: "large"

data:
  corpus: 
  train_shard_urls:
  validation_shard_urls: 
  max_context: 
  full_steps_in_batch:
  checkpoint_directory: 
  bucket_path: 
  index_path_train: 
  index_path_validation: 
  wandb_project: 


device: 
  dp_devices: 
  mp_devices: 