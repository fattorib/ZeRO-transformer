training:
  max_epochs: 10
  batch_size: 128
  peak_learning_rate: 8e-4
  warmup_steps: 2000
  decay_steps: 160000
  total_steps: 180000
  end_learning_rate: 8e-5
  weight_decay: 0.1
  gradient_accumulation_steps: 32
  evaluation_frequency: 500
  maximum_evaluation_steps: 500
  precision: 'bf16'
  staged_warmup_steps: 45000
  warmup_train_context: 128

model:
  size: "smol"

data:
  corpus: 
  train_shard_urls:
  validation_shard_urls: 
  max_context: 2048
  full_steps_in_batch:
  checkpoint_directory: 
  bucket_path: 
  index_path_train: 
  index_path_validation: 
  wandb_project: 


device: 
  dp_devices: 32
  mp_devices: 1