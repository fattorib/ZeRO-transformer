training:
  batch_size: 8
  precision: fp32
  peak_learning_rate: 6e-4
  warmup_steps: 2000
  decay_steps: 10000
  total_steps: 12000
  end_learning_rate: 6e-5
  weight_decay: 0.1
  gradient_accumulation_steps: 8

model:
  size: "small"

data:
  corpus: "books3"