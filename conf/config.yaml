training:
  max_epochs: 8
  batch_size: 128
  peak_learning_rate: 2.7e-4
  warmup_steps: 2000
  decay_steps: 176400
  total_steps: 196000
  end_learning_rate: 2.7e-5
  weight_decay: 0.1
  # corresponds to full CTX BS of 512 on 4 host TPU -> reshape to 512 CTX BS of 1024
  # full batch of 0.5M tokens
  gradient_accumulation_steps: 32
  evaluation_frequency: 500
  maximum_evaluation_steps: 500
  precision: 'bf16'
  staged_sequences: []
  staged_warmup_steps: 1000

model:
  size: "large"

data:
  corpus: "openwebtext"
  train_shard_urls: "data/processed/sharded/openwebtext_train-{000000..000125}.tar.gz"
  validation_shard_urls: "data/processed/sharded/openwebtext_val-{000000..000008}.tar.gz"
  max_context: 1024
  full_steps_in_batch: 24558
  workers: 0
  checkpoint_directory: "checkpoints"
  bucket_path: "bfattoriwebtext2"
  # bucket_path: "bfattoribooks2"
  # bucket_path: 
  index_path_train: "data/index/openwebtext_useast.train.index"
  index_path_validation: "data/index/openwebtext_useast.val.index"
  # index_path_train: "data/index/openwebtext.train.index"
  # index_path_validation: "data/index/openwebtext.val.index"


device:
  dp_devices: 32
  mp_devices: 1