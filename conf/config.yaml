training:
  batch_size: 8
  peak_learning_rate: 6e-4
  warmup_steps: 2000
  decay_steps: 10000
  total_steps: 12000
  end_learning_rate: 6e-5
  weight_decay: 0.1
  gradient_accumulation_steps: 32
  evaluation_frequency: 500
  maximum_evaluation_steps: 500
  precision: 'fp16'

model:
  size: "small"

data:
  corpus: "bookcorpus2"
  train_shard_urls: "data/processed/books_train-{000000..000013}.tar.gz"
  validation_shard_urls: "data/processed/bookcorpus_val-{000000..000002}.tar.gz"
  max_context: 1024
  workers: 1