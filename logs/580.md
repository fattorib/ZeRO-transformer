# 580M Parameter GPT Model Training Log

This training log provides an overview of the pretraining for a 580M parameter GPT2-style model. This model was the first model trained using my Pseudo-ZeRO implementation in Jax and Flax.

## Dataset 

The training dataset was [OpenWebText](https://huggingface.co/datasets/openwebtext) processed from [datasets](https://huggingface.co/docs/datasets/index). The text was tokenized using the Byte-Level [GPTNeoX tokenizer](https://huggingface.co/docs/transformers/model_doc/gpt_neox#transformers.GPTNeoXTokenizerFast). Sequences were tokenized and an end-of-text token was appended to the end of documents. During training sequences of 1024 contiguous tokens were drawn from the dataset. The total dataset consists of 9.02B tokens with Train/Validation splits: 

| Split      | Tokens (B) |
|------------|------------|
| Train      | 8.24       |
| Validation | 0.782      |


## Model 

The model is a 580M parameter decoder only transformer:

| Hyperparameter      | Value |
|---------------------|-------|
| n_parameters        | 580M  |
| n_layers            | 18    |
| d_model             | 1536  |
| d_ff                | 6144  |
| num_head            | 12    |
| d_head              | 128   |
| vocab_size          | 50304 |
| Positional Encoding | ALiBi |
| n_ctx               | 1024  |

This model parameter count places it halfway between GPT2-Medium and GPT2-Large. ALiBi position embeddings were selected as they enable generalization to sequences longer than ones seen during training. 

## Training 

The model was trained for 97000 steps (approx. 48B tokens) with the following hyperparameters:

| Hyperparameter       | Value        |
|----------------------|--------------|
| Batch Size           | 0.5M Tokens  |
| Peak Learning Rate   | 2.5e-4       |
| Warmup Steps         | 2000         |
| Residual Dropout     | 0.1          |
| Attention Dropout    | 0.1          |
| Embedding Dropout    | 0.0          |
| Precision            | bfloat16     |
| Weight Decay         | 0.1          |
| Optimizer            | AdamW        |
| Schedule             | Cosine to 10%|

Training was conducted over 4 days on a TPU v3-32 pod. The model was trained using data parallel training and sharded optimizer states. The model parameters were duplicated across all 32 TPU cores. Due to implementation constraints with ```pmap```, each TPU host kept a duplicated copy of the optimizer states which was sharded across all local devices. 

The model was trained with bf16 activations and float32 params. 


Exact state repository state during training can be recovered at git hash ```28710ba39746598371890c50945596434502c3a4``` 

## Train/Validation perplexity curves during training:
![](/logs/imgs/traincurves.png)

At the end of training, the final model parameters were constructed from averaging together 4 model checkpoints taken during the last 20% of training steps. On a few preliminary PPL benchmarks, this averaged model scored 1-2% better. 

## Results
Despite training for over 5 epochs, validation loss continued to decrease. For the most fair comparison, we compare the model results to GPT2-Medium/GPT2-Large. A priori, we should expect to see GPT-580M outperform GPT2-Medium and underperform relative to GPT2-Large.

**To normalize against the different tokenizers, we score loss/perplexity-style tasks using bits-per-UTF8 encoded byte (BPB)**:


The following metrics were computed with [lm-eval-harness](https://github.com/EleutherAI/lm-evaluation-harness):

| Model       | LAMBADA (ACC) | LAMBADA (PPL) | PIQA (ACC) | Hellaswag (ACC)
|-------------|---------------|---------------|------------|------------|
| GPT2-M | 43.0          | 18.26         | 67.6       | 33.31 |
| **GPT-580M**  | **46.4**        | **14.08**       | **66.8**     | **33.33** |
| GPT2-L | 47.6          | 12.97         | 70.3       | 36.40 |

Subsets of The Pile (GPT2 results from [The Pile](https://arxiv.org/abs/2101.00027))

| Model       | Pile CC (BPB) | OpenWebText2 (BPB) | Pile Wikipedia (BPB) | Arxiv (BPB) | BookCorpus2 (BPB) |
|-------------|---------------|--------------------|----------------------|-------------|-------------------|
| GPT2-M |     0.9992          |    1.0073                |    1.0213                   |   1.2305          |  1.0498                 |
| **GPT-580M**    | **0.9635**        | **0.9159**             | **0.9846**               | **1.3127**      | **1.0249**           |
| GPT2-Large | 0.9582  | 0.9539            | 0.9795         | 1.1778     | 1.0061 |


On LAMBADA, the 580M model performs as expected, while on PIQA, the model falls short of GPT2-Medium. One reason for this could be a discrepancy between the number of training epochs for the models. I was unable to find any conclusive results details how many epochs the GPT2 models were trained for, some sources papers (ex: [here](https://arxiv.org/abs/1906.06669)) seem to imply GPT2 was trained for 20 or 100 epochs, but no source is provided for this claim. 

**Note**: To save on runtime, the Pile evaluations were computed with a maximum of 3000 documents per category. Full evaluations computed on the whole training sets may differ slightly. 

### Sources
- Pile BPB from [here](https://arxiv.org/abs/2101.00027)
- LAMBADA/PIQA from [here](https://github.com/EleutherAI/gpt-neo)

## Errors Made

### 1. Bfloat16 Softmax 

Due to a mistake in the model code, during the forward pass, the attention softmax was computed in bfloat16. For reasons which are still not totally clear to me, this resulted in the model performing poorly on a select subset of benchmarks (LAMBADA PPL and enwiki BPB, for example). Stranger, however, was that this model failure did not occur uniformly. Despite having a BPB of ~40 on enwik8, the model still attained a BPB of ~1.12 on text8 and a LAMBADA accuracy of ~45. 

I was able to fix this model failure by reloading the final converged checkpoint and performing a small amount of training (500 steps) with the attention softmaxes computed in float32. This was sufficient to resolve the problem and fix the broken perplexity score. 

### 2. Mixed Precision Training 

I originally had planned to maintain the model parameters in bfloat16 as well. 

However, when checkpointing the model parameters, all arrays were cast to numpy before serialization. Given that numpy has no knowledge of bfloat16, all model parameters were cast to float32. Upon resuming from a checkpoint, the resume code did not recast the model parameters to bfloat16.

Due to this misstep, the first 10% of training was conducted using bfloat16 activations **and** parameters while the remaining 90% of training was conducted with bfloat16 activations only. 
