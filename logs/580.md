# 580M Parameter GPT Model Training Log

This training log provides an overview of the pretraining for a 580M parameter GPT2-style model. This model was the first model trained using my Pseudo-ZeRO implementation in Jax and Flax.

## Dataset 

The training dataset was [OpenWebText](https://huggingface.co/datasets/openwebtext) processed from [datasets](https://huggingface.co/docs/datasets/index). The text was tokenized using the Byte-Level [GPTNeoX tokenizer](https://huggingface.co/docs/transformers/model_doc/gpt_neox#transformers.GPTNeoXTokenizerFast). Sequences were tokenized and an end-of-text token was appended to the end of documents. During training sequences of 1024 contiguous tokens were drawn from the dataset. The total dataset consists of 9.02B tokens with Train/Validation splits: 

| Split      | Tokens (B) |
|------------|------------|
| Train      | 8.24       |
| Validation | 0.782      |


## Model 

The model is a 580M parameter decoder only transformer:

| Hyperparameter      | Value |
|---------------------|-------|
| n_parameters        | 580M  |
| n_layers            | 18    |
| d_model             | 1536  |
| d_ff                | 6144  |
| num_head            | 12    |
| d_head              | 128   |
| vocab_size          | 50304 |
| Positional Encoding | ALiBi |
| n_ctx               | 1024  |

This model parameter count places it halfway between GPT2-Medium and GPT2-Large. ALiBi position embeddings were selected as they enable generalization to sequences longer than ones seen during training. 

## Training 

The model was trained for 97000 steps (approx. 48B tokens) with the following hyperparameters:

| Hyperparameter       | Value        |
|----------------------|--------------|
| Batch Size           | 0.5M Tokens  |
| Peak Learning Rate   | 2.5e-4       |
| Warmup Steps         | 2000         |
| Residual Dropout     | 0.1          |
| Attention Dropout    | 0.1          |
| Embedding Dropout    | 0.0          |
| Precision            | bfloat16     |
| Weight Decay         | 0.1          |
| Optimizer            | AdamW        |
| Schedule             | Cosine to 10%|

Training was conducted over 4 days on a TPU v3-32 pod. The model was trained using data parallel training and sharded optimizer states. The model parameters were duplicated across all 32 TPU cores. Due to implementation constraints with ```pmap```, each TPU host kept a duplicated copy of the optimizer states which was sharded across all local devices. 

Exact state repository state during training can be recovered at git hash ```28710ba39746598371890c50945596434502c3a4``` 

Train/Validation perplexity curves during training:
![](logs/imgs/traincurves.png)

At the end of training, the final model parameters were constructed from averaging together 4 model checkpoints taken during the last 20% of training steps. On a few preliminary PPL benchmarks, this averaged model scored 1-2% better. 

## Results
Despite training for over 5 epochs, validation loss continued to decrease. For the most fair comparison, we compare the model results to GPT2-Medium/GPT2-Large. A priori, we should expect to see GPT-580M outperform GPT2-Medium and underperform relative to GPT2-Large.

*To normalize against the different tokenizers, we score loss/perplexity-style tasks using bits-per-UTF8 encoded byte (BPB)*:

| Model       | WikiText2 (BPB) | text8 (BPB) |
|-------------|-----------------|-------------|
| GPT2-Medium | 1.0003          | 1.1024      |
| GPT-580M    | 0.9635          | 1.1278      |
| GPT2-Large  | 0.9578          | 1.0543      |

Other tasks (conducted with [lm-eval-harness](https://github.com/EleutherAI/lm-evaluation-harness)):

| Model       | LAMBADA (ACC) | PIQA (ACC) |
|-------------|---------------|------------|
| GPT2-Medium | 43.0        | 67.6     |
| GPT-580M    | 44.7        | 66.5     |
| GPT2-Large  | 47.6         | 70.3     |

On WikiText2 and LAMBADA, the 580M model performs as expected, while on text8 and PIQA, the model falls short of GPT2-Medium. One reason for this could be a discrepancy between the number of training epochs for the models. I was unable to find any conclusive results details how many epochs the GPT2 models were trained for, some sources papers (ex: [here](https://arxiv.org/abs/1906.06669)) seem to imply GPT2 was trained for 20 or 100 epochs, but no source is provided for this claim. 

On other tasks aside from WikiText and text8, the model's perplexity score were significantly larger than expected. Even on LAMBADA, where the model's token accuracy is relatively high, it's perplexity is 1227 signaling that something is not right. In addition, I noted similar results on enwik8 where the model achieves a BPB score of 40 (yes, really). As of this commit, I have been unable to find an explanation for this. I have explored the following areas:
- Errors in evaluation code: On other models I tested, *all* benchmarks are consistent
- Errors/differences between tokenizers: This [Pythia](https://huggingface.co/EleutherAI/pythia-125m) model also uses the NeoX tokenizer. Its performance on enwik8 and LAMBADA PPL is consistent. 
- Errors performing inference in a lower precision: Given that this model is trained in BF16, evaluating in a mix of fp16/fp32 could cause issues. To counter this, for evaluation, all inference is performed in full precision. 
