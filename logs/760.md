# 780M Parameter GPT Model Training Log

This log provides an overview of my pretraining for a 760M parameter GPT2-style model. The model was trained using "warm starting" by scaling up the weights from the 580M param trained checkpoint (see: [here](/logs/580.md)). Between these two runs, the codebase was updated to fix the bfloat16 errors found during the previous run. In addition, a larger dataset incorporating new sources of text was also added. 


## Warm Starting 

We follow Section G.3.3 from [Scaling Language Models: Methods, Analysis
& Insights from Training Gopher](https://arxiv.org/abs/2112.11446) and scale up the fully-converged 580M param model checkpoint to a roughly 760M param model by duplicating a subset of the layers, a 1.5X scale up. Warm-starting a model by only scaling depth-wise was shown to be the most effective method that the authors attempted. They demonstrate this by showing a 40% compute reduction by training a 9B model warm-started from a 4.5B model. Unless mentioned all warm-starting procedures follow the author's guidelines. 

~~Note: I originally attempted to train a 1.1B model (2X scale up), however, even with sharded optimizer states, the model was unable to fit on the TPU v3-32 I was allocated.~~ Reducing the maximum context works (see: [here](/logs/1B.md)).

## Dataset

To the original OpenWebText dataset, I add the following datasets:

| Dataset      | Size (GB) |
|--------------|-----------|
| PhilPapers   | 2         |
| Books2       | 6         |
| Books3 (25%) | 25        |
| PG19         | 10        |

The text was tokenized using the Byte-Level [GPTNeoX tokenizer](https://huggingface.co/docs/transformers/model_doc/gpt_neox#transformers.GPTNeoXTokenizerFast). Sequences were tokenized and an end-of-text token was appended to the end of documents. During training sequences of 1024 contiguous tokens were drawn from the dataset. The total dataset consists of approximately 20B tokens. 

## Model

As mentioned above, the model retains the same structure as the 580M param model, but is 1.5X the size. This corresponds to changing ```n_layers``` from ```18``` to ```24``` in the model config.

## Training 

The model was trained for 82000  steps (approx. 41B tokens) with the following hyperparameters:

| Hyperparameter       | Value        |
|----------------------|--------------|
| Batch Size           | 0.5M Tokens  |
| Peak Learning Rate   | 2.0e-4       |
| Warmup Steps         | 2000         |
| Residual Dropout     | 0.1          |
| Attention Dropout    | 0.1          |
| Embedding Dropout    | 0.0          |
| Precision            | bfloat16     |
| Weight Decay         | 0.1          |
| Optimizer            | AdamW        |
| Schedule             | Cosine to 10%|


In total, the model training took 87 hours on a TPU v3-32 pod. Like it's sibling, the model was trained using data parallel training and sharded optimizer states. The model parameters were duplicated across all 32 TPU cores. Due to implementation constraints with ```pmap```, each TPU host kept a duplicated copy of the optimizer states which was sharded across all local devices. 

Note: *Once validation loss begain to plateau, I manually decreased the learning rate and let the model train for 5B more tokens before finishing the run.*

## Train/Validation perplexity curves during training:
![](/logs/imgs/traincurves_760.png)

## Results

Given the increased dataset mix, we should expect this model to perform *at least* as well as GPT2-Large, if not better. Across the majority of benchmarks, this is what we observe

| Model      | LAMBADA (ACC) | LAMBADA (PPL) | PIQA (ACC) | Hellaswag (ACC) |
|------------|---------------|---------------|------------|-----------------|
| GPT2-Large | 47.6          | 12.97         | 70.3       | -               |
| **GPT-760M**   | **58.0**         | **6.96**          | **70.08**      | **36.62**           |
| GPT2-XL    | 51.21         | 10.63         | 70.78      | 40.03           |

Given that I reprocessed sections of The Pile myself, to avoid any train/test overlap, the model is benchmarked/compared on different components of The Pile not seen during training. (GPT2 results from [The Pile](https://arxiv.org/abs/2101.00027)). The following metrics were computed with [lm-eval-harness](https://github.com/EleutherAI/lm-evaluation-harness):

| Model      | Pile CC (BPB) | OpenWebText2 (BPB) | Pile Wikipedia (BPB) | Arxiv (BPB) | BookCorpus2 (BPB) |
|------------|---------|-------------------|----------------|------------|------------|
| GPT2-Large | 0.9582  | 0.9539            | 0.9795         | 1.1778     | 1.0061 |
| **GPT-760M**   | **0.9193**  | **0.8834**            | **0.9483**         | **1.2362**     | **0.9382** |
| GPT2-XL    | 0.9355      |  0.9171                 |   0.9655             | 1.1381           | 0.9783 |

**Note**: To save on runtime, the Pile evaluations were computed with a maximum of 3000 documents per category. Full evaluations computed on the whole training sets may differ slightly. 

### Sources
- Pile BPB from [here](https://arxiv.org/abs/2101.00027)
- LAMBADA/PIQA/Hellaswag from [here](https://github.com/EleutherAI/gpt-neo)
