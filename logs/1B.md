# 1.1B Parameter GPT Model Training Log

This log provides an overview of my pretraining for a 1.1BM parameter GPT2-style model. The model was trained using "warm starting" by scaling up the weights from the 580M param trained checkpoint (see: [here](/logs/580.md)). The training details of this run are almost identical to the [760M parameter run](/logs/760.md) (including the dataset, hyperparameters and training code) so we only highlight the differences:

## Differences:
This model was trained with a maximum context of 512 tokens. Training at a context of 1024 tokens introduced memory fragmentation errors I was unable to fix. During inference, a context of 1024 was always used. 

## Train/Validation perplexity curves during training:
![](/logs/imgs/traincurves_1B.png)

## Results

Given the increased dataset mix, we should expect this model to perform between GPT-Large and GPT2-XL, if not better. Across the majority of benchmarks, this is what we observe:

| Model       | LAMBADA (ACC) | LAMBADA (PPL) | PIQA (ACC) | Hellaswag (ACC) |
|-------------|---------------|---------------|------------|-----------------|
| GPT2-L      | 47.6          | 12.97         | 70.3       | 36.40               |
| **GPT-1.1B**    | **62.24**         | **5.74**          | **69.42**      | **38.52**           |
| GPT2-XL     | 51.21         | 10.63         | 70.78      | 40.03           |

Given that I reprocessed sections of The Pile myself, to avoid any train/test overlap, the model is benchmarked/compared on different components of The Pile not seen during training. (GPT2 results from [The Pile](https://arxiv.org/abs/2101.00027)). The following metrics were computed with [lm-eval-harness](https://github.com/EleutherAI/lm-evaluation-harness):

| Model       | Pile CC (BPB) | OpenWebText2 (BPB) | Pile Wikipedia (BPB) | Arxiv (BPB) | BookCorpus2 (BPB) |
|-------------|---------------|--------------------|----------------------|-------------|-------------------|
| GPT2-L      | 0.9582        | 0.9539             | 0.9795               | 1.1778      | 1.0061            |
| **GPT-1.1B**    | **0.9319**        | **0.8778**             | **0.9301**               | **1.2444**      | **0.9286**            |
| GPT2-XL     | 0.9355        |  0.9171            |   0.9655             | 1.1381      | 0.9783            |

**Note**: The LAMBADA scores are suspiciously good. The copy of BookCorpus2 I processed was not deduplicated against LAMBADA, in addition, the corpus was seen approximately two times during training. There is almost certainly some data-leakage happening here. Take these scores with a grain of salt. 

**Note**: To save on runtime, the Pile evaluations were computed with a maximum of 3000 documents per category. Full evaluations computed on the whole training sets may differ slightly. 

### Sources
- Pile BPB from [here](https://arxiv.org/abs/2101.00027)
- LAMBADA/PIQA/Hellaswag from [here](https://github.com/EleutherAI/gpt-neo)
